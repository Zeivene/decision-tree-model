# -*- coding: utf-8 -*-
"""Decision_tree_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/181nVE2-GeNjfVsSjIX9vocMmzH9VcLL1

# **Python**

**import libraries**
"""

import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
import graphviz
from sklearn.tree import export_graphviz
from IPython.display import display
from sklearn.metrics import confusion_matrix
import seaborn as sns
from sklearn.metrics import accuracy_score 
from sklearn.metrics import precision_score,recall_score,f1_score
import matplotlib.pyplot as plt

"""# **Decision Tree from scratch**

**Entropy Function**
"""

def Entropy(target_col):
    elements,counts = np.unique(target_col,return_counts = True)  
    entropy=0
    for i in range(len(elements)):
        entropy += (-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts))
    
    return entropy

"""**Gain Function**"""

def gain(data,s_feature,target_feature):       
        n_entropy = 0
        vals,counts= np.unique(data[s_feature],return_counts=True)
        for i in range(len(vals)):
            k=Entropy(data.where(data[s_feature]==vals[i]).dropna()[target_feature])
            n_entropy += (counts[i]/np.sum(counts))*k
        return n_entropy

"""**Information Gain Function**"""

def Infogain(df,s_feature,y):
    total_entropy = Entropy(df[y])
  
    Information_Gain = total_entropy - gain(df,s_feature,y)
    
    return Information_Gain

"""**Split Information Function**"""

def split_info(data,f):
    elements,counts = np.unique(data[f],return_counts = True)
    entropy=0
    for i in range(len(elements)):
        entropy += (-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts))
    return entropy

"""**Gain Ratio Function**"""

def Gain_ratio(info_gain,data,best_feature):
    ratio =  info_gain/split_info(data,best_feature)
    return ratio

"""**Decision Tree Function**"""

def DT(data,originaldata,features,target_feature="target",parent = None,level=0): 
    
 
    if (len(np.unique(data[target_feature])) <= 1):   # checking if all the values are same if yes then we reached at a leaf
        
        elements,counts = np.unique(data[target_feature],return_counts = True)
        print('Level ',level)
        if elements ==0 :
            print('Count of 0 =',np.sum(counts))
        elif elements ==1 :
            print('Count of 1 =',np.sum(counts))
        elif elements ==2 :
            print('Count of 2 =',np.sum(counts))
        print('Current Entropy is =',Entropy(data[target_feature]))
        print('Reached Leaf Node ')
        print()
        return np.unique(data[target_feature])[0]
    
    elif len(data) == 0:                              # checking the data is empty or not
       
        return np.unique(originaldata[target_feature])
    
    elif len(features) == 0 :
        
        return parent
    
    else :

        P_node = np.unique(data[target_feature])      # put all the uniqe values of target in parent node
        
        values = []
        for ftr in features :                         # loop over all the features
            v = Infogain(data,ftr,target_feature)     # getting list of information gain of all features
            values.append(v)
        
        best_feature_index = np.argmax(values)        # taking out the index of the feture which contains max information gain
        best_feature = features[best_feature_index]
        
        tree = {best_feature:{}}                      # i have used dictionaris to show my actual tree
        
        tot_entropy = Entropy(data[target_feature])   # calculated entropy at current node
       
        
        rat=Gain_ratio(max(values),data,best_feature) # calculated gain ratio  of the features on which we split up on
        
        elements,counts = np.unique(data[target_feature],return_counts = True)
        print('Level ',level)                                      #these all are printing task
        for i in range(len(elements)):
            if elements[i]==0 :
                print('count of 0  =',counts[i])
            elif elements[i]==1 :
                print('count of 1  =',counts[i])
            elif elements[i]==2 :
                print('count of 2  =',counts[i])
      
        print('Current entropy is   = ',tot_entropy)  
        print('Splitting on feature ',best_feature,' with gain ratio ', rat)
        
        print()
        
        
        new_features = features                     # ---> from here to  
        features=[]
        for i in new_features :                     
                                                   #  process to remove the feature from feature list after split
            if i != best_feature :                  
                features.append(i)
        level += 1       
        new_features=None                          # ---> to here
        
        for vals in np.unique(data[best_feature]):   #   recursion of all diffrent values in that splitting feature
            
            value = vals
            sub_data = (data[data[best_feature] == value]).dropna()
            
            subtree = DT(sub_data,data,features,target_feature,P_node,level)
            tree[best_feature][value] = subtree
            
        return tree

"""**Loading & Labeling Iris Dataset**


"""

# its all given to us , i have just added one more column of target

iris = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data", header=None, sep=',')
# iris = datasets.load_iris()
print(iris)
df1 = pd.DataFrame(iris)
df=df1.drop([4],axis=1)
df.columns = ["sepal_length", "sepal_width", 'petal_length', 'petal_width']
y = pd.DataFrame(df1[4])
y.columns = ['target']


def label(val, *boundaries):
    if (val < boundaries[0]):
        return 'a'
    elif (val < boundaries[1]):
        return 'b'
    elif (val < boundaries[2]):
        return 'c'
    else:
        return 'd'

#Function to convert a continuous data into labelled data
#There are 4 lables  - a, b, c, d
def toLabel(df, old_feature_name):
    second = df[old_feature_name].mean()
    minimum = df[old_feature_name].min()
    first = (minimum + second)/2
    maximum = df[old_feature_name].max()
    third = (maximum + second)/2
    return df[old_feature_name].apply(label, args= (first, second, third))

#Convert all columns to labelled data
df['sepal_length_labeled'] = toLabel(df, 'sepal_length')
df['sepal_width_labeled'] = toLabel(df, 'sepal_width')
df['petal_length_labeled'] = toLabel(df, 'petal_length')
df['petal_width_labeled'] = toLabel(df, 'petal_width')

df.head()
df.drop(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], axis = 1, inplace = True)    
df['target']=y                                                    
print(df.columns[:-1])

"""**Printing steps of decition tree as given in Problem**"""

tree=DT(df, df, df.columns[:-1])     # printing steps
print()

"""**Printing Actual Tree with two types**"""

import pprint  
pprint.pprint(tree)         # printing actual tree (type 1)

def formatData(t,s):
    if not isinstance(t,dict) and not isinstance(t,list):
        print ("\t"*s+str(t))
    else:
        for key in t:
            print ("\t"*s+str(key))
            if not isinstance(t,list):
                formatData(t[key],s+1)

formatData(tree,0)                          # printing actual tree (type 2)

"""# **Implement Decision Tree from Sklearn**"""

df_sklearn = pd.DataFrame(iris)
df_sklearn.columns = ["sepal_length", "sepal_width", 'petal_length', 'petal_width',"target"]
df_sklearn.head()

sns.countplot(x='target', data=df_sklearn, )
plt.show()

sns.scatterplot(x='sepal_length', y='sepal_width',
                hue='target', data=df_sklearn, )
 
# Placing Legend outside the Figure
plt.legend(bbox_to_anchor=(1, 1), loc=2)
 
plt.show()

sns.pairplot(df_sklearn,hue='target')

x=df_sklearn.drop(['target'],axis=1)
y=df_sklearn['target']

# import the function
from sklearn.model_selection import train_test_split

# Split our data into the training sets and the testing sets
# Each set has a pair of X and y
X_train, X_test, y_train, y_test = train_test_split(x, y,test_size=0.25, random_state=1)

print("the shape of x_train is ",X_train.shape)
print("the shape of y_train is ",y_train.shape)
print("the shape of x_test is ",X_test.shape)
print("the shape of y_test is ",y_test.shape)

## Import and create the model

dt = DecisionTreeClassifier()

# Train our model with our training data.
dt.fit(X_train, y_train)

display(graphviz.Source(export_graphviz(dt)))

y_pred = dt.predict(X_test)

cm=confusion_matrix(y_pred, y_test)

sns.heatmap(cm,annot=True)

print("The precision  is ",precision_score(y_test, y_pred,average="weighted"))
print("The recall is ",recall_score(y_test, y_pred,average="weighted"))
print("The f1_score  is ",f1_score(y_test, y_pred,average="weighted"))

print("the Accuracy is ",accuracy_score(y_test, y_pred))

"""**Hyperparameter tuning**"""

from sklearn.model_selection import GridSearchCV
params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [5, 10, 20, 50, 100],
    'criterion': ["gini", "entropy"]
}

# Instantiate the grid search model
grid_search = GridSearchCV(estimator=dt, 
                           param_grid=params, 
                           cv=4, n_jobs=-1, verbose=1, scoring = "accuracy")

grid_search.fit(X_train, y_train)

score_df = pd.DataFrame(grid_search.cv_results_)
score_df.head()

score_df.nlargest(5,"mean_test_score")

grid_search.best_estimator_

dt_best = grid_search.best_estimator_

display(graphviz.Source(export_graphviz(dt_best)))

from sklearn.metrics import classification_report
print(classification_report(y_test, dt_best.predict(X_test)))

cm=confusion_matrix(dt_best.predict(X_test), y_test)

sns.heatmap(cm,annot=True)

print("The precision  is ",precision_score(y_test, dt_best.predict(X_test),average="weighted"))
print("The recall is ",recall_score(y_test, dt_best.predict(X_test),average="weighted"))
print("The f1_score  is ",f1_score(y_test, dt_best.predict(X_test),average="weighted"))

"""**Testing**"""

import pytest

def test_decision_tree_output():
    # Test input/output pairs
    assert dt.predict([[5, 3, 4, 2]]) == [1]

def test_decision_tree_structure():
    # Check the number of nodes
    assert dt.tree_.node_count == 15

    # Check the correctness of the first split
    assert dt.tree_.feature[0] == 3
    assert dt.tree_.threshold[0] == 2.45

"""**Reference**

1. https://www.geeksforgeeks.org/exploratory-data-analysis-on-iris-dataset/

2. https://medium.com/@bressan/software-engineering-for-data-scientists-testing-ml-models-with-pytest-d28b83ab54d9


"""